{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attribution.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skrb33/language-exercise/blob/main/notebooks/Attribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RCK3vzYIoG04"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone repo to access resources"
      ]
    },
    {
      "metadata": {
        "id": "I7YpL1Jzgjro",
        "outputId": "3564a27b-3ee0-4044-b650-f06d5d6cf4ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/idealo/cnn-exposed.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cnn-exposed'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 58 (delta 0), reused 0 (delta 0), pack-reused 55 (from 1)\u001b[K\n",
            "Receiving objects: 100% (58/58), 71.30 MiB | 39.29 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "S2vNVFJxj4e7",
        "outputId": "87d7f841-e9bc-4ae6-b0d7-7413f28a8b24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "cd cnn-exposed/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'cnn-exposed/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "21IyywKT4GEu"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "D-S3w0X_kXZC"
      },
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import os\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from skimage import feature, transform\n",
        "from matplotlib.pyplot import figure\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# import keras dependencies\n",
        "from keras.models import Model\n",
        "from keras.applications import MobileNet as CNN\n",
        "from keras.applications.mobilenet import preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dR-gA14j6oPc"
      },
      "cell_type": "markdown",
      "source": [
        "### Declare some helper functions"
      ]
    },
    {
      "metadata": {
        "id": "mkSHCTHxkd9D"
      },
      "cell_type": "code",
      "source": [
        "def plot_single_image(image_path, fig_size=(10, 10), dpi=100):\n",
        "    figure(figsize=fig_size, dpi=dpi)\n",
        "    img = mpimg.imread(image_path)\n",
        "    plt.imshow(img)\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plotting function for saliency maps\n",
        "def plot_custom(data, xi=None, cmap='RdBu_r', axis=plt, percentile=100, dilation=3.0, alpha=0.8):\n",
        "    dx, dy = 0.05, 0.05\n",
        "    xx = np.arange(0.0, data.shape[1], dx)\n",
        "    yy = np.arange(0.0, data.shape[0], dy)\n",
        "    xmin, xmax, ymin, ymax = np.amin(xx), np.amax(xx), np.amin(yy), np.amax(yy)\n",
        "    extent = xmin, xmax, ymin, ymax\n",
        "    cmap_xi = plt.get_cmap('Greys_r')\n",
        "    cmap_xi.set_bad(alpha=0)\n",
        "    overlay = None\n",
        "    if xi is not None:\n",
        "        # Compute edges (to overlay to heatmaps later)\n",
        "        xi_greyscale = xi if len(xi.shape) == 2 else np.mean(xi, axis=-1)\n",
        "        in_image_upscaled = transform.rescale(xi_greyscale, dilation, mode='constant')\n",
        "        edges = feature.canny(in_image_upscaled).astype(float)\n",
        "        edges[edges < 0.5] = np.nan\n",
        "        edges[:5, :] = np.nan\n",
        "        edges[-5:, :] = np.nan\n",
        "        edges[:, :5] = np.nan\n",
        "        edges[:, -5:] = np.nan\n",
        "        overlay = edges\n",
        "\n",
        "    abs_max = np.percentile(np.abs(data), percentile)\n",
        "    abs_min = -abs_max\n",
        "\n",
        "    if len(data.shape) == 3:\n",
        "        data = np.mean(data, 2)\n",
        "    axis.imshow(data, extent=extent, interpolation='bicubic', cmap=cmap, vmin=abs_min, vmax=abs_max)\n",
        "    if overlay is not None:\n",
        "        axis.imshow(overlay, extent=extent, interpolation='bicubic', cmap=cmap_xi, alpha=alpha)\n",
        "    axis.axis('off')\n",
        "    return axis\n",
        "\n",
        "\n",
        "def plot_comparison(target_image_path, map_array, title=''):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(26, 20))\n",
        "\n",
        "    img_orig = Image.open(target_image_path).resize((224, 224))\n",
        "    xi = (map_array[0,:] - np.min(map_array[0,:]))\n",
        "    xi /= np.max(xi)\n",
        "\n",
        "    ax = axes.flatten()[0]\n",
        "    ax.imshow(img_orig)\n",
        "    ax.set_title('Original', fontdict={'fontsize': 20})\n",
        "    ax.axis('off')\n",
        "\n",
        "    plot_custom(attributions[0], xi = xi, axis=axes[1], dilation=.5, percentile=99, alpha=.2).set_title(title, fontdict={'fontsize': 20})\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_gradcam(original_image_path, loaded_image, grads):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(16, 10), constrained_layout=True)\n",
        "\n",
        "    img_orig = Image.open(original_image_path).resize((224, 224))\n",
        "    axs[0].imshow(img_orig, aspect='auto')\n",
        "    axs[0].grid(False)\n",
        "\n",
        "    axs[1].imshow(overlay(grads, loaded_image), aspect='auto')\n",
        "    axs[1].grid(False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYneEaynkfMT"
      },
      "cell_type": "code",
      "source": [
        "path_resources = pathlib.Path('resources/attribution/')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yyOTgxl8kkD7"
      },
      "cell_type": "markdown",
      "source": [
        "# Attribution approaches\n",
        "\n",
        "Several approaches:\n",
        "\n",
        "\n",
        "1.   **Perturbation based approaches** - Occlude area of interest to test its effectiveness in changing predictions\n",
        "2.   **Gradient bases approaches** - Calculation of gradients of output w.r.t. some network variable\n",
        "  * *Saliency Maps*\n",
        "  * Guided Backpropagation\n",
        "  * Deconvolution\n",
        "3.   **Relevance Score approaches**\n",
        "  * *Class Activation Map (CAM)*\n",
        "  * *Grad- CAM*\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "G51Kx2AGknx9"
      },
      "cell_type": "markdown",
      "source": [
        "# Image classification\n",
        "We wish to classify the below image with true label for the dog breed: Australian Terrier"
      ]
    },
    {
      "metadata": {
        "id": "MBPuSHEQkkti",
        "outputId": "4a7c424b-fda7-4153-daf9-6466153a70fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "cell_type": "code",
      "source": [
        "target_image = os.path.join(path_resources, 'inp_im.png')\n",
        "plot_single_image(target_image)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'resources/attribution/inp_im.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2985816893.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inp_im.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4-2970885766.py\u001b[0m in \u001b[0;36mplot_single_image\u001b[0;34m(image_path, fig_size, dpi)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             )\n\u001b[0;32m-> 1502\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1504\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/attribution/inp_im.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "XJ6hSQt6kwOn"
      },
      "cell_type": "markdown",
      "source": [
        "# Saliency Maps\n",
        "## Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n",
        "by Simonyan, Vedaldi, Zisserman\n",
        "\n",
        "(https://arxiv.org/pdf/1312.6034v2.pdf)\n",
        "\n",
        "### Take gradient of output w.r.t each input"
      ]
    },
    {
      "metadata": {
        "id": "dQ_n5XsPkm-b",
        "outputId": "8f34d320-1d66-4c48-82e0-841ac9dff3f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "cell_type": "code",
      "source": [
        "saliency_diagram = os.path.join(path_resources, 'saliency.png')\n",
        "plot_single_image(saliency_diagram, fig_size=(18, 18))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'resources/attribution/saliency.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-3581913330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msaliency_diagram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_resources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saliency.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaliency_diagram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4-2970885766.py\u001b[0m in \u001b[0;36mplot_single_image\u001b[0;34m(image_path, fig_size, dpi)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_single_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             )\n\u001b[0;32m-> 1502\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1504\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/attribution/saliency.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x1800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "cQ3z36OXk1S7"
      },
      "cell_type": "markdown",
      "source": [
        "## Python package: DeepExplain\n",
        "(https://github.com/marcoancona/DeepExplain#egg=deepexplain)"
      ]
    },
    {
      "metadata": {
        "id": "sIkFeAMKkyCc",
        "outputId": "9b38c36f-458c-48d1-8dcb-161a81a520d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the model (Mobilenet Pretrained on ImageNet dataset)\n",
        "model = CNN(include_top=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "brP8WU2EkzJw",
        "outputId": "8a65cc70-241c-44c8-d833-f2ae57c350a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the prediction\n",
        "loaded_image = np.array(image.load_img(target_image, target_size=(224, 224)))\n",
        "processed_image = preprocess_input(loaded_image)\n",
        "preds = model.predict(processed_image[np.newaxis, :])\n",
        "preds_name = decode_predictions(preds)\n",
        "preds_name"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'resources/attribution/inp_im.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-2577424303.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprocessed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpreds_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/attribution/inp_im.png'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GH3vKvnClGPO"
      },
      "cell_type": "markdown",
      "source": [
        "## Install Deepexplain package for plotting saliency maps"
      ]
    },
    {
      "metadata": {
        "id": "pFFARxTWltqd",
        "outputId": "32648f99-71a9-4c5e-e957-da2779e4c541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/marcoancona/DeepExplain.git#egg=deepexplain"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepexplain\n",
            "  Cloning https://github.com/marcoancona/DeepExplain.git to /tmp/pip-install-960zfwlq/deepexplain_392ca7d889f64b4c90baa3de5e76eba1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/marcoancona/DeepExplain.git /tmp/pip-install-960zfwlq/deepexplain_392ca7d889f64b4c90baa3de5e76eba1\n",
            "  Resolved https://github.com/marcoancona/DeepExplain.git to commit 87fb43a13ac2a3b285a030b87df899cc40100c94\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from deepexplain) (1.15.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from deepexplain) (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from deepexplain) (0.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->deepexplain) (2.9.0.post0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->deepexplain) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->deepexplain) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->deepexplain) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->deepexplain) (0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->deepexplain) (1.17.0)\n",
            "Building wheels for collected packages: deepexplain\n",
            "  Building wheel for deepexplain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepexplain: filename=deepexplain-0.3-py3-none-any.whl size=15246 sha256=03b70ddc113a622e7ac9f55a06a388b9085875486557d4ed6a842f586dc8a9ce\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4lhqjlft/wheels/c6/3b/f1/c77c05e206a49abec9107da7bad879af005231a97eccc1aa39\n",
            "Successfully built deepexplain\n",
            "Installing collected packages: deepexplain\n",
            "Successfully installed deepexplain-0.3\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zX-mB8fLkzPO"
      },
      "cell_type": "code",
      "source": [
        "# import deepexplain to draw saliency maps\n",
        "from deepexplain.tensorflow import DeepExplain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKGzMo7tlOlh"
      },
      "cell_type": "code",
      "source": [
        "# Get saliency map\n",
        "# Refer the API documentation for using deepexplain as below\n",
        "with DeepExplain(session=K.get_session()) as de:\n",
        "    model = CNN(include_top=True)\n",
        "    input_tensor = model.layers[0].input\n",
        "    fModel = Model(inputs=input_tensor, outputs = model.layers[-1].output)\n",
        "    target_tensor = fModel(input_tensor)\n",
        "    top_idx = preds.argsort()[::-1]\n",
        "    ys = to_categorical(top_idx, num_classes=1000)\n",
        "    xs = np.tile(processed_image, (1, 1, 1, 1))\n",
        "    attributions = de.explain('saliency', fModel.outputs[0] * ys, fModel.inputs[0], xs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRlE_ev-lOnm"
      },
      "cell_type": "code",
      "source": [
        "plot_comparison(target_image, attributions, title='Saliency map')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qveAmSCZm_Ju"
      },
      "cell_type": "markdown",
      "source": [
        "## Saliency Map Pros:\n",
        "\n",
        "*   Fine-grained understanding of relative contribution of pixels\n",
        "\n",
        "## Saliency Map Cons:\n",
        "* Propagation of gradient is hard (due to non-linearlities such as relu which makes the gradient discontinuous)"
      ]
    },
    {
      "metadata": {
        "id": "7WrARO3MnB1_"
      },
      "cell_type": "markdown",
      "source": [
        "# Class Activation Map (CAM)\n",
        "## Learning Deep Features for Discriminative Localization\n",
        "by Zhou et. al\n",
        "\n",
        "(http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "J5q1qRTkm9ie"
      },
      "cell_type": "code",
      "source": [
        "cam_diagram = os.path.join(path_resources, 'cam.png')\n",
        "plot_single_image(cam_diagram, fig_size=(15, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gd2u3SYLnGt2"
      },
      "cell_type": "markdown",
      "source": [
        "## CAM Pros:\n",
        "\n",
        "*   No calculation of gradients needed\n",
        "*   Intuitive to understand\n",
        "\n",
        "## CAM Cons:\n",
        "* Bound to a fixed architecture (Conv -> GAP -> Dense)"
      ]
    },
    {
      "metadata": {
        "id": "88lyD3zPnK1v"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Class Activation Map (Grad-CAM)\n",
        "\n",
        "## Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\n",
        "\n",
        "by Selvaraju et.al (https://arxiv.org/pdf/1610.02391.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "9tKlSLjqm-NT"
      },
      "cell_type": "code",
      "source": [
        "gradcam_diagram = os.path.join(path_resources, 'gradcam.png')\n",
        "plot_single_image(gradcam_diagram, fig_size=(14, 14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1pmY_DPm-PY",
        "outputId": "f293812e-5481-4081-b24b-4e44d2daf65f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "# Summary of the MobilNet model (pretrained on ImageNet dataset) that was loaded earlier\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-4180436154.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Summary of the MobilNet model (pretrained on ImageNet dataset) that was loaded earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "XQpDo_axnS4s"
      },
      "cell_type": "markdown",
      "source": [
        "## Install keras-vis package for obtaining grad-cam\n",
        "\n",
        "(https://github.com/raghakot/keras-vis)"
      ]
    },
    {
      "metadata": {
        "id": "U0rHlH4AnQPV",
        "outputId": "52d42bd5-e00d-4344-d33d-db33adeb2b21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras-vis"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-vis\n",
            "  Downloading keras_vis-0.4.1-py2.py3-none-any.whl.metadata (757 bytes)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.14.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from keras-vis) (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from keras-vis) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from keras-vis) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py->keras-vis) (2.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras->keras-vis) (24.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->keras-vis) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (1.15.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->keras-vis) (0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-vis) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-vis) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-vis) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-vis) (0.1.2)\n",
            "Downloading keras_vis-0.4.1-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: keras-vis\n",
            "Successfully installed keras-vis-0.4.1\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DjKxg3nFnQRe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "ad38760a-6bb0-4dfc-c118-c7755b699b44"
      },
      "cell_type": "code",
      "source": [
        "# import specific functions from keras-vis package\n",
        "from vis.utils import utils\n",
        "from vis.visualization import visualize_cam, overlay"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Iterable' from 'collections' (/usr/lib/python3.11/collections/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1036877845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import specific functions from keras-vis package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_cam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vis/utils/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Iterable' from 'collections' (/usr/lib/python3.11/collections/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Apcx4WJBnft2"
      },
      "cell_type": "code",
      "source": [
        "# Get layer for which Grad-CAM needs to be obtained ('conv_preds' is the name of\n",
        "# the convolution layer closest to the output layer. Refer the output of\n",
        "# model.summary()). Any convolution layer can be visualized (not only the one\n",
        "# closest to the output)\n",
        "layer_idx = utils.find_layer_idx(model, 'conv_preds')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRv8jDNBnfv5"
      },
      "cell_type": "code",
      "source": [
        "grads = visualize_cam(model, layer_idx, filter_indices=np.argmax(preds), seed_input=processed_image, backprop_modifier=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqSOW0pQniCw"
      },
      "cell_type": "code",
      "source": [
        "plot_gradcam(target_image, loaded_image, grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uKrGwMPLnmyQ"
      },
      "cell_type": "markdown",
      "source": [
        "# Layerwise Relevance Propagation (LRP)\n",
        "## On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation\n",
        "by Bach et. al. (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140)"
      ]
    },
    {
      "metadata": {
        "id": "d4UNdjL6niEm"
      },
      "cell_type": "code",
      "source": [
        "lrp_diagram_1 = os.path.join(path_resources, 'lrp_new.png')\n",
        "plot_single_image(lrp_diagram_1, fig_size=(3, 3), dpi=300)\n",
        "print('source: Montavon et. al., Explaining nonlinear classification decisions with deep Taylor decomposition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yW2EsYBHnqc2"
      },
      "cell_type": "code",
      "source": [
        "lrp_diagram_2 = os.path.join(path_resources, 'lrp.png')\n",
        "plot_single_image(lrp_diagram_2, fig_size=(4, 4), dpi=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYOpav4tnsbx"
      },
      "cell_type": "code",
      "source": [
        "# Get Epsilon-LRP heatmap\n",
        "with DeepExplain(session=K.get_session()) as de:\n",
        "    model = CNN(include_top=True)\n",
        "    input_tensor = model.layers[0].input\n",
        "    fModel = Model(inputs=input_tensor, outputs = model.layers[-1].output)\n",
        "    target_tensor = fModel(input_tensor)\n",
        "    top_idx = preds.argsort()[::-1]\n",
        "    ys = to_categorical(top_idx, num_classes=1000)\n",
        "    xs = np.tile(processed_image, (1, 1, 1, 1))\n",
        "    attributions = de.explain('elrp', fModel.outputs[0] * ys, fModel.inputs[0], xs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXYjRXNcnseA"
      },
      "cell_type": "code",
      "source": [
        "plot_comparison(target_image, attributions, title='Epsilon-LRP')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aiVLKl4Gn1Q5"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing out some more images"
      ]
    },
    {
      "metadata": {
        "id": "KxQLtJeSnzdV"
      },
      "cell_type": "code",
      "source": [
        "image_urls = [\n",
        "    'https://aristainflorida.com/wp-content/uploads/2017/11/jay-bird-konar-winter-45212-small.jpg',\n",
        "    'https://cdn.britannica.com/55/31555-131-240223FB.jpg',\n",
        "    'https://i.pinimg.com/originals/60/a1/3f/60a13ffca856faf182b2ee44cfe59f41.jpg',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5W5-Z9Arn4cX"
      },
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for url in image_urls:\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content)).resize((224, 224))\n",
        "    images.append(np.array(image))\n",
        "\n",
        "images_pp = preprocess_input(np.array(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXXFWWSmn4ea"
      },
      "cell_type": "code",
      "source": [
        "# Get predictions\n",
        "preds = model.predict(images_pp)\n",
        "preds_name = decode_predictions(preds)\n",
        "\n",
        "for j, url in enumerate(image_urls):\n",
        "    print('{}'.format(url))\n",
        "    for (i, (imagenetID, label, prob)) in enumerate(preds_name[j]):\n",
        "        print('{}. {}: {:.2f}%'.format(i + 1, label, prob * 100))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HEoMu7B5n86t"
      },
      "cell_type": "code",
      "source": [
        "N_PLOT_PRED = 2 # top-n predictions to plot for each image\n",
        "K.clear_session()\n",
        "\n",
        "with DeepExplain(session=K.get_session()) as de:\n",
        "\n",
        "    model = CNN(include_top=True)\n",
        "    input_tensor = model.layers[0].input\n",
        "\n",
        "  # We target the output of the last dense layer (pre-softmax)\n",
        "  # To do so, create a new model sharing the same layers until the last dense\n",
        "\n",
        "    fModel = Model(inputs=input_tensor, outputs = model.layers[-1].output)\n",
        "    target_tensor = fModel(input_tensor)\n",
        "\n",
        "    for u, url in enumerate(image_urls):\n",
        "        print('{}:'.format(url))\n",
        "        top_idx = preds[u].argsort()[-N_PLOT_PRED:][::-1] # Get indices of the top-2 predicted classes\n",
        "\n",
        "        ys = to_categorical(top_idx, num_classes=1000) # one-hot encode the predicted indices\n",
        "        xs = np.tile(images_pp[u], (N_PLOT_PRED, 1, 1, 1)) # Duplicate the image N_PLOT_PRED number of times\n",
        "\n",
        "        # Draw saliency maps and Epsilon-LRP heatmap\n",
        "        attributions = {\n",
        "            'Saliency maps': de.explain('saliency', fModel.outputs[0] * ys, fModel.inputs[0], xs),\n",
        "            'Epsilon-LRP': de.explain('elrp', fModel.outputs[0] * ys, fModel.inputs[0], xs),\n",
        "        }\n",
        "        print ('Done!')\n",
        "\n",
        "        # Plotting Function\n",
        "        n_cols = int(len(attributions)) + 1\n",
        "        n_rows = 1\n",
        "\n",
        "        for i, xi in enumerate(xs):\n",
        "            print('{}: {:.2f}%'.format(preds_name[u][i][1], preds_name[u][i][2] * 100))\n",
        "\n",
        "            fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(5*n_cols, 5*n_rows))\n",
        "\n",
        "            xi = (xi - np.min(xi))\n",
        "            xi /= np.max(xi)\n",
        "            ax = axes.flatten()[0]\n",
        "            ax.imshow(images[u])\n",
        "            ax.set_title('Original')\n",
        "            ax.axis('off')\n",
        "\n",
        "            for j, a in enumerate(attributions):\n",
        "                axj = axes.flatten()[j + 1]\n",
        "                plot_custom(attributions[a][i], xi = xi, axis=axj, dilation=.5, percentile=99, alpha=.2).set_title(a)\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ko0ej7fnn8Fv"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}